{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac192915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import h5py\n",
    "\n",
    "from utils import *\n",
    "from build_model import *\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_sample = 10000\n",
    "\n",
    "with h5py.File('HHbbbb.h5', 'r') as f:\n",
    "    X_HHbbbb_isHS = np.sum(tf.cast(f['HS'][:n_per_sample], tf.float32), axis=-1, keepdims=True)\n",
    "    X_HHbbbb_isPU = np.sum(tf.cast(f['PU'][:n_per_sample], tf.float32), axis=-1, keepdims=True)\n",
    "\n",
    "with h5py.File('PJZ0.h5', 'r') as f:\n",
    "    X_PJZ0 = np.sum(tf.cast(f['data'][:n_per_sample], tf.float32), axis=-1, keepdims=True)\n",
    "\n",
    "print(X_HHbbbb_isHS.shape)\n",
    "print(X_HHbbbb_isPU.shape)\n",
    "print(X_PJZ0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9428917",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layers(event_idx=1, X=X_HHbbbb_isHS+X_HHbbbb_isPU, label='[HHbbbb, PU=200]', n_layers=1)\n",
    "plot_layers(event_idx=1, X=X_HHbbbb_isHS, label='[HHbbbb, PU=0]', n_layers=1)\n",
    "plot_layers(event_idx=2, X=X_PJZ0, label='[QCD dijet, PU=200]', n_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pu = 100\n",
    "#x_augmented, total_e_before_removal, total_e_after_removal, total_e_scale = augment_pu(image=X_HHbbbb_isPU[0], target_pu=target_pu, shift_phi=False, threshold=1)\n",
    "x_augmented = augment_pu(image=X_HHbbbb_isPU[0], target_pu=target_pu, shift_phi=True, threshold=1)\n",
    "\n",
    "plot_layers(event_idx=None, X=X_HHbbbb_isPU[0], label='[Pure PU, 200]', n_layers=1)\n",
    "plot_layers(event_idx=None, X=x_augmented, label=f'[Pure PU, aug. {target_pu}]', n_layers=1)\n",
    "\n",
    "#print(\"total_e_before_removal: \"+str(total_e_before_removal))\n",
    "#print(\"total_e_after_removal: \"+str(total_e_after_removal))\n",
    "#print(\"total_e_scale: \"+str(total_e_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64ba85",
   "metadata": {},
   "source": [
    "## train vicreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "pu_min = 100\n",
    "pu_max = 200\n",
    "steps_per_epoch = (X_HHbbbb_isHS.shape[0] + X_PJZ0.shape[0]) // batch_size\n",
    "threshold = 1\n",
    "\n",
    "gen_data_contrastive = generate_batch_for_contrastive(X_hs=X_HHbbbb_isHS,\n",
    "                                                      X_pu=X_HHbbbb_isPU,\n",
    "                                                      X_bkg=X_PJZ0,\n",
    "                                                      pu_min=pu_min,\n",
    "                                                      pu_max=pu_max,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      threshold=threshold)\n",
    "\n",
    "encoder = build_encoder(input_shape=input_shape, embedding_dim=embedding_dim)\n",
    "projection_head = build_projection_head(embedding_dim=embedding_dim, projection_dim=projection_dim)\n",
    "\n",
    "vicreg_model = VICRegModel(encoder=encoder, projection_head=projection_head, c_inv=c_inv, c_var=c_var, c_cov=c_cov)\n",
    "vicreg_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.005))\n",
    "\n",
    "encoder.summary()\n",
    "print('\\n\\n')\n",
    "projection_head.summary()\n",
    "\n",
    "history = vicreg_model.fit(gen_data_contrastive, steps_per_epoch=steps_per_epoch, epochs=20)\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "axes = plt.subplot(1,1,1)\n",
    "axes.plot(history.history['loss'], label = 'loss (total)')\n",
    "axes.plot(history.history['loss_inv'], label = 'invariance')\n",
    "axes.plot(history.history['loss_var'], label = 'variance')\n",
    "axes.plot(history.history['loss_cov'], label = 'covariance')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')\n",
    "#axes.set_yscale('log')\n",
    "#axes.set_ylim((0.001, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031430e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('weights_encoder.h5'):\n",
    "    os.remove('weights_encoder.h5')\n",
    "    \n",
    "if os.path.exists('weights_projection_head.h5'):\n",
    "    os.remove('weights_projection_head.h5')\n",
    "\n",
    "encoder.save_weights('weights_encoder.h5')\n",
    "projection_head.save_weights('weights_projection_head.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf014bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_loaded = build_encoder(input_shape=input_shape, embedding_dim=embedding_dim)\n",
    "encoder_loaded.load_weights('weights_encoder.h5')\n",
    "\n",
    "projection_head_loaded = build_projection_head(embedding_dim=embedding_dim, projection_dim=projection_dim)\n",
    "projection_head_loaded.load_weights('weights_projection_head.h5')\n",
    "\n",
    "vicreg_model_loaded = VICRegModel(encoder=encoder_loaded,\n",
    "                                  projection_head=projection_head_loaded,\n",
    "                                  c_inv=c_inv,\n",
    "                                  c_var=c_var,\n",
    "                                  c_cov=c_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f9028",
   "metadata": {},
   "source": [
    "## train embedding classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ffac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "pu_min = 100\n",
    "pu_max = 101\n",
    "steps_per_epoch = (X_HHbbbb_isHS.shape[0] + X_PJZ0.shape[0]) // batch_size\n",
    "threshold = 1\n",
    "\n",
    "gen_data_classification = generate_batch_for_classifier(X_hs=X_HHbbbb_isHS,\n",
    "                                                        X_pu=X_HHbbbb_isPU,\n",
    "                                                        X_bkg=X_PJZ0,\n",
    "                                                        pu_min=pu_min,\n",
    "                                                        pu_max=pu_max,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        threshold=threshold)\n",
    "\n",
    "# no fine tuning for the encoder\n",
    "embedding_classifier_nofinetune = build_embedding_classifier(encoder=encoder_loaded, input_shape=input_shape, encoder_trainable=False)\n",
    "embedding_classifier_nofinetune.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_nofinetune = embedding_classifier_nofinetune.fit(gen_data_classification, steps_per_epoch=steps_per_epoch, epochs=20)\n",
    "\n",
    "# fine tuning for the encoder (tuning phase)\n",
    "embedding_classifier_finetune = build_embedding_classifier(encoder=encoder_loaded, input_shape=input_shape, encoder_trainable=True)\n",
    "embedding_classifier_finetune.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_finetune1 = embedding_classifier_finetune.fit(gen_data_classification, steps_per_epoch=steps_per_epoch, epochs=1)\n",
    "\n",
    "# fine tuning for the encoder (non tuning phase)\n",
    "embedding_classifier_finetune.get_layer('encoder').trainable = False\n",
    "embedding_classifier_finetune.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_finetune2 = embedding_classifier_finetune.fit(gen_data_classification, steps_per_epoch=steps_per_epoch, epochs=19)\n",
    "\n",
    "history_finetune = history_finetune1.history['loss'] + history_finetune2.history['loss']\n",
    "plt.figure(figsize=(6,4))\n",
    "axes = plt.subplot(1,1,1)\n",
    "axes.plot(history_nofinetune.history['loss'], label = 'loss (no fine-tuning)')\n",
    "axes.plot(history_finetune, label='loss (with fine-tuning)')\n",
    "axes.legend(loc=\"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b705580",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('weights_embedding_classifier_nofinetune.h5'):\n",
    "    os.remove('weights_embedding_classifier_nofinetune.h5')\n",
    "\n",
    "if os.path.exists('weights_embedding_classifier_finetune.h5'):\n",
    "    os.remove('weights_embedding_classifier_finetune.h5')\n",
    "\n",
    "embedding_classifier_nofinetune.save_weights('weights_embedding_classifier_nofinetune.h5')\n",
    "embedding_classifier_finetune.save_weights('weights_embedding_classifier_finetune.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_loaded = build_encoder(input_shape=input_shape, embedding_dim=embedding_dim)\n",
    "encoder_loaded.load_weights('weights_encoder.h5')\n",
    "\n",
    "embedding_classifier_nofinetune_loaded = build_embedding_classifier(encoder=encoder_loaded, input_shape=input_shape)\n",
    "embedding_classifier_nofinetune_loaded.load_weights('weights_embedding_classifier_nofinetune.h5')\n",
    "\n",
    "embedding_classifier_finetune_loaded = build_embedding_classifier(encoder=encoder_loaded, input_shape=input_shape)\n",
    "embedding_classifier_finetune_loaded.load_weights('weights_embedding_classifier_finetune.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f15baa",
   "metadata": {},
   "source": [
    "## train standalone classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058081b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "pu_min = 100\n",
    "pu_max = 101\n",
    "steps_per_epoch = (X_HHbbbb_isHS.shape[0] + X_PJZ0.shape[0]) // batch_size\n",
    "threshold = 1\n",
    "\n",
    "gen_data_classification = generate_batch_for_classifier(X_hs=X_HHbbbb_isHS,\n",
    "                                                        X_pu=X_HHbbbb_isPU,\n",
    "                                                        X_bkg=X_PJZ0,\n",
    "                                                        pu_min=pu_min,\n",
    "                                                        pu_max=pu_max,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        threshold=threshold)\n",
    "\n",
    "standalone_classifier = build_standalone_classifier(input_shape=input_shape)\n",
    "standalone_classifier.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.005),\n",
    "                              loss='binary_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "history = standalone_classifier.fit(gen_data_classification, steps_per_epoch=steps_per_epoch, epochs=20)\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "axes = plt.subplot(1,1,1)\n",
    "axes.plot(history.history['loss'], label = 'loss')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb20f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('weights_standalone_classifier.h5'):\n",
    "    os.remove('weights_standalone_classifier.h5')\n",
    "\n",
    "standalone_classifier.save_weights('weights_standalone_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone_classifier_loaded = build_standalone_classifier(input_shape=input_shape)\n",
    "standalone_classifier_loaded.load_weights('weights_standalone_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeccc12",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d3a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_sample = 80000\n",
    "\n",
    "with h5py.File('HHbbbb.h5', 'r') as f:\n",
    "    X_HHbbbb_isHS = tf.cast(f['HS'][-n_per_sample:], tf.float32)\n",
    "    X_HHbbbb_isPU = tf.cast(f['PU'][-n_per_sample:], tf.float32)\n",
    "\n",
    "with h5py.File('PJZ0.h5', 'r') as f:\n",
    "    X_PJZ0 = tf.cast(f['data'][-n_per_sample:], tf.float32)\n",
    "\n",
    "print(X_HHbbbb_isHS.shape)\n",
    "print(X_HHbbbb_isPU.shape)\n",
    "print(X_PJZ0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271df5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pu = list(range(100, 201, 20))\n",
    "X_test = []\n",
    "Y_test = []\n",
    "Y_pred_embedding_nofinetune = []\n",
    "Y_pred_embedding_finetune = []\n",
    "Y_pred_standalone = []\n",
    "threshold = 1\n",
    "for pu in test_pu:\n",
    "    x, y = generate_dataset_for_classifier(X_hs=X_HHbbbb_isHS, X_pu=X_HHbbbb_isPU, X_bkg=X_PJZ0, target_pu=pu, threshold=threshold)\n",
    "    X_test.append(x)\n",
    "    Y_test.append(y)\n",
    "\n",
    "    Y_pred_embedding_nofinetune.append(embedding_classifier_nofinetune_loaded.predict(x))\n",
    "    Y_pred_embedding_finetune.append(embedding_classifier_finetune_loaded.predict(x))\n",
    "    Y_pred_standalone.append(standalone_classifier_loaded.predict(x))\n",
    "    print(pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(Y_test=Y_test, Y_pred_embedding=Y_pred_embedding_nofinetune, Y_pred_standalone=Y_pred_standalone, test_pu=test_pu)\n",
    "#plot_sig_eff_vs_pu_at_single_bkgeff(Y_test=Y_test, Y_pred_embedding=Y_pred_embedding_nofinetune, Y_pred_standalone=Y_pred_standalone, test_pu=test_pu, bkg_eff_list=[0.05,0.1,0.2,0.5])\n",
    "plot_eff_vs_pu_at_single_threshold(Y_test=Y_test, Y_pred_embedding=Y_pred_embedding_nofinetune, Y_pred_standalone=Y_pred_standalone, test_pu=test_pu, threshold_by_target_bkgeff_pu=[0.001, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e10e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(Y_test=Y_test, Y_pred_embedding=Y_pred_embedding_finetune, Y_pred_standalone=Y_pred_standalone, test_pu=test_pu)\n",
    "#plot_sig_eff_vs_pu_at_single_bkgeff(Y_test=Y_test, Y_pred_embedding=Y_pred_embedding_finetune, Y_pred_standalone=Y_pred_standalone, test_pu=test_pu, bkg_eff_list=[0.05,0.1,0.2,0.5])\n",
    "plot_eff_vs_pu_at_single_threshold(Y_test=Y_test, Y_pred_embedding=Y_pred_embedding_finetune, Y_pred_standalone=Y_pred_standalone, test_pu=test_pu, threshold_by_target_bkgeff_pu=[0.001, 100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
