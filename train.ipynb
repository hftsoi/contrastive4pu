{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac192915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "import h5py\n",
    "\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_sample = 10000\n",
    "\n",
    "with h5py.File('HHbbbb.h5', 'r') as f:\n",
    "    X_HHbbbb_isHS = tf.cast(f['HS'][:n_per_sample], tf.float32)\n",
    "    X_HHbbbb_isPU = tf.cast(f['PU'][:n_per_sample], tf.float32)\n",
    "\n",
    "with h5py.File('PJZ0.h5', 'r') as f:\n",
    "    X_PJZ0 = tf.cast(f['data'][:n_per_sample], tf.float32)\n",
    "\n",
    "print(X_HHbbbb_isHS.shape)\n",
    "print(X_HHbbbb_isPU.shape)\n",
    "print(X_PJZ0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9428917",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layers(event_idx=1, X=X_HHbbbb_isHS+X_HHbbbb_isPU, label='[HHbbbb, PU=200]')\n",
    "plot_layers(event_idx=1, X=X_HHbbbb_isHS, label='[HHbbbb, PU=0]')\n",
    "plot_layers(event_idx=2, X=X_PJZ0, label='[QCD dijet, PU=200]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pu = 100\n",
    "x_augmented = augment_pu(image=X_HHbbbb_isPU[0], target_pu=target_pu, shift_phi=True)\n",
    "\n",
    "plot_layers(event_idx=None, X=X_HHbbbb_isPU[0], label='[Pure PU, 200]')\n",
    "plot_layers(event_idx=None, X=x_augmented, label=f'[Pure PU, aug. {target_pu}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a837c52",
   "metadata": {},
   "source": [
    "## build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64, 50, 6)\n",
    "embedding_dim = 128\n",
    "projection_dim = 64\n",
    "c_inv = 25\n",
    "c_var = 25\n",
    "c_cov = 1\n",
    "\n",
    "def build_encoder(input_shape=input_shape, embedding_dim=embedding_dim):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(embedding_dim)(x)\n",
    "    outputs = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def build_projection_head(embedding_dim=embedding_dim, projection_dim=projection_dim):\n",
    "    inputs = tf.keras.Input(shape=(embedding_dim,))\n",
    "    x = tf.keras.layers.Dense(embedding_dim, activation='relu')(inputs)\n",
    "    outputs = tf.keras.layers.Dense(projection_dim)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"projection_head\")\n",
    "\n",
    "\n",
    "def vicreg_loss(z1, z2, c_inv=c_inv, c_var=c_var, c_cov=c_cov, epsilon=0.0001):\n",
    "    # invariance between positive views\n",
    "    loss_inv = tf.reduce_mean(tf.square(z1 - z2))\n",
    "\n",
    "    # maximize variance per feature dim across sample batch (avoid collapse--learning constant vector)\n",
    "    std_z1 = tf.sqrt(tf.math.reduce_variance(z1, axis=0) + epsilon)\n",
    "    std_z2 = tf.sqrt(tf.math.reduce_variance(z2, axis=0) + epsilon)\n",
    "    loss_var = tf.reduce_mean(tf.nn.relu(1 - std_z1)) + tf.reduce_mean(tf.nn.relu(1 - std_z2))\n",
    "\n",
    "    # minimize covariance between feature dims (to reduce learning feature redundancy)\n",
    "    z1_centered = z1 - tf.reduce_mean(z1, axis=0)\n",
    "    z2_centered = z2 - tf.reduce_mean(z2, axis=0)\n",
    "    batch_size = tf.cast(tf.shape(z1)[0], tf.float32)\n",
    "    # covariance matrices for z1 z2\n",
    "    cov_z1 = tf.matmul(tf.transpose(z1_centered), z1_centered) / (batch_size - 1)\n",
    "    cov_z2 = tf.matmul(tf.transpose(z2_centered), z2_centered) / (batch_size - 1)\n",
    "    # get diagonal parts\n",
    "    diag_z1 = tf.linalg.diag(tf.linalg.diag_part(cov_z1))\n",
    "    diag_z2 = tf.linalg.diag(tf.linalg.diag_part(cov_z2))\n",
    "    # subtract diagonal parts from cov matrices to get off-diagonal parts (cov between features)\n",
    "    loss_cov_z1 = tf.reduce_sum(tf.square(cov_z1 - diag_z1)) / tf.cast(tf.shape(z1)[1], tf.float32)\n",
    "    loss_cov_z2 = tf.reduce_sum(tf.square(cov_z2 - diag_z2)) / tf.cast(tf.shape(z2)[1], tf.float32)\n",
    "    loss_cov = loss_cov_z1 + loss_cov_z2\n",
    "\n",
    "    loss_inv *= c_inv\n",
    "    loss_var *= c_var\n",
    "    loss_cov *= c_cov\n",
    "\n",
    "    loss = loss_inv + loss_var + loss_cov\n",
    "\n",
    "    return loss, loss_inv, loss_var, loss_cov\n",
    "\n",
    "\n",
    "class VICRegModel(tf.keras.Model):\n",
    "    def __init__(self, encoder, projection_head, c_inv=c_inv, c_var=c_var, c_cov=c_cov, **kwargs):\n",
    "        super(VICRegModel, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.projection_head = projection_head\n",
    "        self.c_inv = c_inv\n",
    "        self.c_var = c_var\n",
    "        self.c_cov = c_cov\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        super(VICRegModel, self).compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # view1, view2\n",
    "        x1, x2 = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            emb1 = self.encoder(x1, training=True)\n",
    "            emb2 = self.encoder(x2, training=True)\n",
    "\n",
    "            z1 = self.projection_head(emb1, training=True)\n",
    "            z2 = self.projection_head(emb2, training=True)\n",
    "\n",
    "            loss, loss_inv, loss_var, loss_cov = vicreg_loss(z1, z2,\n",
    "                                                             c_inv=self.c_inv,\n",
    "                                                             c_var=self.c_var,\n",
    "                                                             c_cov=self.c_cov)\n",
    "        \n",
    "        vars = self.encoder.trainable_variables + self.projection_head.trainable_variables\n",
    "        grads = tape.gradient(loss, vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, vars))\n",
    "\n",
    "        return {\"loss\": loss, \"loss_inv\": loss_inv, \"loss_var\": loss_var, \"loss_cov\": loss_cov}\n",
    "\n",
    "\n",
    "def build_classifier(encoder, input_shape=input_shape):\n",
    "    # update or freeze the encoder weights\n",
    "    encoder.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    # option-training here concerns about training/inference mode in dropout, batchnorm etc. \n",
    "    x = encoder(inputs, training=False)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64ba85",
   "metadata": {},
   "source": [
    "## train vicreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "pu_min = 100\n",
    "pu_max = 200\n",
    "steps_per_epoch = (X_HHbbbb_isHS.shape[0] + X_PJZ0.shape[0]) // batch_size\n",
    "\n",
    "gen_data_contrastive = generate_batch_for_contrastive(X_hs=X_HHbbbb_isHS,\n",
    "                                                      X_pu=X_HHbbbb_isPU,\n",
    "                                                      X_bkg=X_PJZ0,\n",
    "                                                      pu_min=pu_min,\n",
    "                                                      pu_max=pu_max,\n",
    "                                                      batch_size=batch_size)\n",
    "\n",
    "encoder = build_encoder(input_shape=input_shape, embedding_dim=embedding_dim)\n",
    "projection_head = build_projection_head(embedding_dim=embedding_dim, projection_dim=projection_dim)\n",
    "\n",
    "vicreg_model = VICRegModel(encoder=encoder, projection_head=projection_head, c_inv=c_inv, c_var=c_var, c_cov=c_cov)\n",
    "vicreg_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0005))\n",
    "\n",
    "encoder.summary()\n",
    "print('\\n\\n')\n",
    "projection_head.summary()\n",
    "\n",
    "history = vicreg_model.fit(gen_data_contrastive, steps_per_epoch=steps_per_epoch, epochs=40)\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "axes = plt.subplot(1,1,1)\n",
    "axes.plot(history.history['loss'], label = 'loss (total)')\n",
    "axes.plot(history.history['loss_inv'], label = 'invariance')\n",
    "axes.plot(history.history['loss_var'], label = 'variance')\n",
    "axes.plot(history.history['loss_cov'], label = 'covariance')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031430e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('weights_encoder.h5'):\n",
    "    os.remove('weights_encoder.h5')\n",
    "    \n",
    "if os.path.exists('weights_projection_head.h5'):\n",
    "    os.remove('weights_projection_head.h5')\n",
    "\n",
    "encoder.save_weights('weights_encoder.h5')\n",
    "projection_head.save_weights('weights_projection_head.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf014bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_loaded = build_encoder(input_shape=input_shape, embedding_dim=embedding_dim)\n",
    "encoder_loaded.load_weights('weights_encoder.h5')\n",
    "\n",
    "projection_head_loaded = build_projection_head(embedding_dim=embedding_dim, projection_dim=projection_dim)\n",
    "projection_head_loaded.load_weights('weights_projection_head.h5')\n",
    "\n",
    "vicreg_model_loaded = VICRegModel(encoder=encoder_loaded,\n",
    "                                  projection_head=projection_head_loaded,\n",
    "                                  c_inv=c_inv,\n",
    "                                  c_var=c_var,\n",
    "                                  c_cov=c_cov)\n",
    "vicreg_model_loaded.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0005))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f9028",
   "metadata": {},
   "source": [
    "## train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "pu_min = 100\n",
    "pu_max = 200\n",
    "steps_per_epoch = (X_HHbbbb_isHS.shape[0] + X_PJZ0.shape[0]) // batch_size\n",
    "\n",
    "gen_data_classification = generate_batch_for_classifier(X_hs=X_HHbbbb_isHS,\n",
    "                                                        X_pu=X_HHbbbb_isPU,\n",
    "                                                        X_bkg=X_PJZ0,\n",
    "                                                        pu_min=pu_min,\n",
    "                                                        pu_max=pu_max,\n",
    "                                                        batch_size=batch_size)\n",
    "\n",
    "classifier = build_classifier(encoder=encoder, input_shape=input_shape)\n",
    "classifier.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0005),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history = classifier.fit(gen_data_classification, steps_per_epoch=steps_per_epoch, epochs=20)\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "axes = plt.subplot(1,1,1)\n",
    "axes.plot(history.history['loss'], label = 'loss')\n",
    "axes.plot(history.history['accuracy'], label = 'accuracy')\n",
    "axes.legend(loc = \"upper right\")\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b705580",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('weights_classifier.h5'):\n",
    "    os.remove('weights_classifier.h5')\n",
    "\n",
    "classifier.save_weights('weights_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_loaded = build_encoder(input_shape=input_shape, embedding_dim=embedding_dim)\n",
    "encoder_loaded.load_weights('weights_encoder.h5')\n",
    "\n",
    "classifier_loaded = build_classifier(encoder=encoder_loaded, input_shape=input_shape)\n",
    "classifier_loaded.load_weights('weights_classifier.h5')\n",
    "classifier.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0005),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38d1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdeccc12",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2424df7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
